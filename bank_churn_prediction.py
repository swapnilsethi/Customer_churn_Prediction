# -*- coding: utf-8 -*-
"""BANK CHURN PREDICTION.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x7O6g-OXXxqeh4GxGTRFWzLxcfRrtc0Q
"""

import warnings
warnings.filterwarnings('ignore')

# Import libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import joblib

"""# Loading Dataset"""

# Read Dataset
df = pd.read_csv('data.csv')
df.head()

"""# Describing the dataset"""

df.describe()

# We can see that this is an unbalanced dataset

"""# Datatype of the features"""

# Datatypes of Features
df.info()

"""# Checking Null value"""

# Check for null values
df.isnull().sum()

"""# Checking duplicate values"""

#Check duplicate records
df.duplicated().sum()

"""# Finding Unique features"""

# Count unique value in each feature
df.nunique()

#As we know rownumber, customeerID and Surname has nothing to do with the customer churn or not. So, let's remove these columns.

df.drop(['RowNumber','CustomerId','Surname'],axis=1,inplace=True)
df.head()

#Finding the shape of the dataset

df.shape

# Checking distribution of CreditScore
sns.boxplot(df.CreditScore)
plt.show()

#From above we can see that, there are few records with very low credit score. We will see later does this lower credit score is directly related with chrun or not.

# Checking distribution of Age
sns.boxplot(df.Age)
plt.show()

# Check the percentage of records in different geographical locations.
df.groupby(['Geography']).size().plot(kind="pie",stacked=True,figsize=(6, 6),table=True,title="Geographical Distribution",autopct='%1.1f%%')
plt.show()

# Here we can see Geographical Distribution, More than half of the customers are from France.

# Distribution as per Gender
df.groupby(['Gender']).size().plot(kind="pie",stacked=True,figsize=(6, 6),table=True,title="Gender Diversity",autopct='%1.1f%%')
plt.show()

# This is Gender Diversity pie chart, we can conclude that there are more male customers than the female cusomers

# Percentage of records having credit card  
df.groupby(['HasCrCard']).size().plot(kind="pie",stacked=True,figsize=(6, 6),table=True,title="People having Credit Card",autopct='%1.1f%%')
plt.show()

# From the graph we can see that we have 70% of customers who have Credit Card

# Distribution of customers as per number of prodcuts they are using
df.groupby(['NumOfProducts']).size().plot(kind="pie",stacked=True,figsize=(6, 6),table=True,title="No.Of Products",autopct='%1.1f%%')
plt.show()

# In this we can observe that most number of customers has enrolled for only 1 product.

#Distribution of customers they churned or not.
df.groupby(['Exited']).size().plot(kind="pie",stacked=True,figsize=(6, 6),table=True,autopct='%1.1f%%')
labels = ["Loyal Customer","Churn Customers"]
plt.show()

# 20% of customers are churning. This also indicate that dataset is inbalanced.

ExitedValues = df.Exited.value_counts()
labels = ["Loyal Customer","Churn Customers"]
fig1, f1 = plt.subplots()
f1.pie(ExitedValues,labels=labels, autopct='%1.1f%%') 
plt.tight_layout()
plt.show()

fig, ax = plt.subplots()
fig.set_size_inches(11.7, 8.27)
sns.set(font_scale = 0.75)
sns.heatmap(df.corr(), annot = True, fmt = ".6f")
plt.show()

# From above heatmap we can see that Age is one of the main factor of churn. Also,member is active or not is also main factor in churn.

# Number of Customer Existed
sns.countplot(df.Exited).set_title('No.of Customer Exited')
plt.show()

"""# Defining dependent and independent varibles"""

x = df.iloc[:,:-1]

x.head()

y =  df.iloc[:,-1:]

y.head()

"""# Categorical values and Numerical Values"""

cat_col = []
num_col = []
for col in df.columns:
    if df[col].dtype=="O":
        cat_col.append(col)
    else:
        num_col.append(col)

print("Object data type features ",cat_col)
print("Numerical data type features ",num_col)

"""# Using LabelEncoder """

# I used Label Encoder as its Ordinal Datatype

from sklearn.preprocessing import LabelEncoder

laen = LabelEncoder()
laen.fit_transform(df['Gender'])
x['Gender_label'] = laen.fit_transform(df['Gender'])

x.head()

x['Gender_label'].value_counts()

x['Gender'].value_counts()

x.head()

"""# Using OneHotEnoder"""

from sklearn.preprocessing  import OneHotEncoder

Geography_lables = pd.get_dummies(x['Geography'])

Geography_lables

x.join(Geography_lables)

x = x.join(Geography_lables)

x.head()

"""# Dropping unwanted Features"""

x.drop(['Geography','Gender','Spain'],axis=1,inplace=True)

x.head()

"""# Train-Test Split"""

from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import f1_score

x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=42)

"""# Standarization of the data"""

from sklearn.preprocessing import StandardScaler
sc_x=StandardScaler()
x_train=sc_x.fit_transform(x_train)
x_test=sc_x.fit_transform(x_test)

"""# LOGISTIC REGRESSION"""

lor=LogisticRegression()

lor.fit(x_train, y_train)

"""# Cross Validation Score"""

cv_score = cross_val_score(lor,x_train,y_train,cv=5)
np.mean(cv_score)

y_pred=lor.predict(x_test)

from sklearn.metrics import confusion_matrix
cm= confusion_matrix(y_test,y_pred)

cm

"""# Accuracy"""

TP, TN, FP, FN = 1541, 80, 66,313
Accuracy = (TP + TN)/(TP + TN + FP + FN)
print('Accuracy:',Accuracy*100)

"""# Recall"""

TP = 1541
FN = 313

recall = TP / (TP + FN)*100
print(f"recall: {recall:4.2f}")

"""# Precision"""

TP = 1541
FP = 66

precision = TP / (TP + FP)*100
print(f"precision: {precision:4.2f}")

"""# RANDOM FOREST"""

from sklearn.ensemble import RandomForestClassifier

rfc = RandomForestClassifier(max_depth=5, random_state=42, max_leaf_nodes=50)

rfc.fit(x_train,y_train)

"""# Cross Validation Score"""

cv_score = cross_val_score(rfc,x_train,y_train,cv=5)
np.mean(cv_score)

y_pred1 = rfc.predict(x_test)

print(classification_report(y_test,y_pred1))

f1_score(y_test,y_pred1)

confusion_matrix(y_pred1,y_test)

"""# Accuracy"""

TP, TN, FP, FN = 1586, 113, 280,21
Accuracy = (TP + TN)/(TP + TN + FP + FN)
print("Accuracy:",Accuracy*100)

"""# Recall"""

TP = 1586
FN = 21

recall = TP / (TP + FN)*100
print(f"recall: {recall:4.2f}")

"""# Precision"""

TP = 1586
FP = 280

precision = TP / (TP + FP)*100
print(f"precision: {precision:4.2f}")

"""# KNN"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn import metrics

knnclassifier = KNeighborsClassifier(n_neighbors=5)
knnclassifier.fit(x_train,y_train)

"""# Cross Validation Score"""

knnclassifier = KNeighborsClassifier(n_neighbors=4)
print(cross_val_score(knnclassifier, x, y, cv=5, scoring ='accuracy').mean())

"""# Accuracy"""

from sklearn.metrics import confusion_matrix
cm= confusion_matrix(y_test,y_pred)

cm

TP, TN, FP, FN = 1508, 141, 99,252
Accuracy = (TP + TN)/(TP + TN + FP + FN)
print("Accuracy:",Accuracy*100)

"""# Recall"""

TP = 1508
FN = 252

recall = TP / (TP + FN)*100
print(f"recall: {recall:4.2f}")

"""# Precision"""

TP = 1508
FP = 99

precision = TP / (TP + FP)*100
print(f"precision: {precision:4.2f}")

x

"""# Prediction data"""

rfc.predict([[300,25,5,85000.04,1,0,4,10522.00,1,1,0]])

rfc.predict([[620,19,1,5000.04,1,0,4,1522.00,1,1,0]])

model_filename = 'model.pkl'
joblib.dump(rfc,model_filename)

